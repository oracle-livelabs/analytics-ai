{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35edd987",
   "metadata": {},
   "source": [
    "### T5-Base Untuned\n",
    "\n",
    "Let's try the model first directly without tunning it to see what results we would get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239ca44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datascience/conda/pytorch110_p38_gpu_v1/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We're loading the T5 wrappers to assist with loading the model from HuggingFace\n",
    "# Then we load the T5 tokenizer we've used previously\n",
    "# Next, we're going to use a pipeline, which is a wrapper from the Transformers library, to help us use this model and the tokenizer for text generation\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the T5 without our fine tuning\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", model_max_len=512)\n",
    "\n",
    "t5_generator = pipeline(model=t5_model, tokenizer=t5_tokenizer, task=\"text2text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb4e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hard-coded list of groceries which we will pass to the model as a parameter\n",
    "\n",
    "prompt = 'cooking oats, brown sugar, salad oil, eggs, salt, almond extract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16687e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe: cooking oats, brown sugar, brown sugar, brown sugar, eggs, salt, almond extract, salt, almond extract, almond extract, salt, almond extract, almond extract, salt, almond extract, almond extract, salt, almond extract, almond extract, salt, almond extract, almond extract, salt, almond extract, almond extract, almond extract, almond extract, almond extract, almond extract, almond extract, almond extract, almond extract, almond extract, almond extract, almond extract, almond extract, almond extract, almond extract, almond\n"
     ]
    }
   ],
   "source": [
    "# Send a prompt to the model which includes our request and the grocery list\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    'max_length':512\n",
    "}\n",
    "\n",
    "response = t5_generator(f\"generate recipe: {prompt}\", **tokenizer_kwargs)\n",
    "\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7269ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch110_p38_gpu_v1]",
   "language": "python",
   "name": "conda-env-pytorch110_p38_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
