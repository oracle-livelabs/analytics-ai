{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46478ebf-2a4d-4f1d-86ab-0d7b8530eed4",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "## This notebook ingest the raw \"races\" from the bronze layer to silver layer\n",
    "### Ingest f1_races_dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b39202-68f6-4088-8bc9-d6dec7ee9d56",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Define parameters (can set parameters in a workflow job)\n",
    "target_type   =oidlUtils.parameters.getParameter(\"TARGET_TYPE\", \"table\")\n",
    "target_format =oidlUtils.parameters.getParameter(\"TARGET_FORMAT\", \"delta\")\n",
    "bronze_catalog    = \"f1_bronze\"\n",
    "silver_catalog    = \"f1_silver\"\n",
    "bronze_schema     = \"bronze\"\n",
    "silver_schema     = \"silver\"\n",
    "bronze_table_dlt = \"f1_races_dlt\"\n",
    "bronze_table_par = \"f1_races_par\"\n",
    "silver_table_dlt = \"f1_races_dlt\"\n",
    "silver_table_par = \"f1_races_par\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37412d61-6d65-430a-925e-d4a7c665617d",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "# ----------\n",
    "##### Step 1 - Read Bronze table to dataframe, select and rename columns \n",
    "# ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5a485-9c7e-4ecf-9099-47a2aab967bf",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "races_df = spark.read.table(f\"{bronze_catalog}.{bronze_schema}.{bronze_table_dlt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0117e34-b2c3-4606-9783-60919b863a46",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp\n",
    "races_selected_df = races_df.select(col('RACEID').alias('race_id'), col('YEAR').alias('race_year'), col('ROUND').alias('round'),\n",
    "                                    col('NAME').alias('name'), col('DATE').alias('race_date'), col('TIME').alias('race_time'),\n",
    "                                    col('CIRCUITREF').alias('circuit_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef88782-94ea-49a2-9a4e-779fe6b8e23d",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "# ----------\n",
    "##### Step 2 - Change time format, Add ingestion date and race_timestamp to the dataframe\n",
    "# ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d7549-46d1-4806-baf1-402273f86155",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, to_timestamp, concat, col, lit, lpad, length, split, concat_ws, expr, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eabfe0-530d-41ea-b836-e970ad74814f",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Clean and pad the time string to always be in HH:mm:ss format\n",
    "races_with_clean_time_df = races_selected_df.withColumn(\n",
    "    \"clean_race_time\",\n",
    "    when(col(\"race_time\").isNull() | (col(\"race_time\") == \"\") | (col(\"race_time\").rlike(r\"^\\s*$\")), lit(\"00:00:00\"))\n",
    "    .otherwise(\n",
    "        expr(\"\"\"\n",
    "            format_string(\n",
    "                '%02d:%02d:%02d',\n",
    "                int(split(race_time, ':')[0]),\n",
    "                int(split(race_time, ':')[1]),\n",
    "                int(split(race_time, ':')[2])\n",
    "            )\n",
    "        \"\"\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db136cc-03b5-46e4-924f-58660414f85e",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "races_with_timestamp_df = races_with_clean_time_df.withColumn(\"ingestion_date\", current_timestamp()) \\\n",
    "                                  .withColumn(\"race_timestamp\", to_timestamp(concat(col('race_date'), lit(' '), col('clean_race_time')), 'dd-MMM-yy HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf02bbe-ecb8-4ffd-acf9-bc0eac9424c8",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "# ----------\n",
    "##### Step 3 - Select final columns required\n",
    "# ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d151e2-6a16-4358-80f8-05c6a1ce8b2e",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "races_selected_df = races_with_timestamp_df.select(col('race_id'), col('race_year'), col('round'), \n",
    "                                                   col('circuit_id'), col('name'), col('ingestion_date'), col('race_timestamp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24db39-fca2-4ee5-b9b3-71d44e4a223b",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "# ----------\n",
    "##### Step 4 - Write the output to processed container in delta/parquet table or parquet file format\n",
    "# ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8cd16-f147-4ddd-bee6-3b091abeb427",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "if target_type == 'file':\n",
    "    if target_format == 'parquet':\n",
    "        races_selected_df.write.mode(\"overwrite\").parquet(f\"{silver_folder_path}/races\")\n",
    "elif target_type == 'table':\n",
    "    if target_format == 'parquet':\n",
    "        races_selected_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(f\"{silver_catalog}.{silver_schema}.{silver_table_par}\")\n",
    "    elif  target_format == 'delta':\n",
    "        races_selected_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{silver_catalog}.{silver_schema}.{silver_table_dlt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20ddc9-f77a-4dba-b51c-bd83098652bd",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "if target_type == 'file':\n",
    "    if target_format == 'parquet':\n",
    "        races_read_df = spark.read.parquet(f\"{silver_folder_path}/races\")\n",
    "elif target_type == 'table':\n",
    "    if target_format == 'parquet':\n",
    "        races_read_df = spark.read.table(f\"{silver_catalog}.{silver_schema}.{silver_table_par}\")\n",
    "    elif  target_format == 'delta':\n",
    "        races_read_df = spark.read.table(f\"{silver_catalog}.{silver_schema}.{silver_table_dlt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b0eab-9743-4a63-8f85-2e054f7e6bbf",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "races_read_df.show()"
   ]
  }
 ],
 "metadata": {
  "Last_Active_Cell_Index": 14,
  "kernelspec": {
   "name": "notebook"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  },
  "title": "New Untitled_1744724325561_ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
