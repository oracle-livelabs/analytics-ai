{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46478ebf-2a4d-4f1d-86ab-0d7b8530eed4",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "## This notebook ingest the raw \"results\" from the bronze layer to silver layer\n",
    "### Ingest f1_results_dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b39202-68f6-4088-8bc9-d6dec7ee9d56",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "# Define parameters (can set parameters in a workflow job)\n",
    "target_type   =oidlUtils.parameters.getParameter(\"TARGET_TYPE\", \"table\")\n",
    "target_format =oidlUtils.parameters.getParameter(\"TARGET_FORMAT\", \"delta\")\n",
    "bronze_catalog    = \"f1_bronze\"\n",
    "silver_catalog    = \"f1_silver\"\n",
    "bronze_schema     = \"bronze\"\n",
    "silver_schema     = \"silver\"\n",
    "bronze_table_dlt = \"f1_results_dlt\"\n",
    "bronze_table_par = \"f1_results_par\"\n",
    "silver_table_dlt = \"f1_results_dlt\"\n",
    "silver_table_par = \"f1_results_par\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37412d61-6d65-430a-925e-d4a7c665617d",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "# ----------\n",
    "##### Step 1 - Read Bronze table to dataframe. select columns, rename and cast type (if needed). \n",
    "# ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5a485-9c7e-4ecf-9099-47a2aab967bf",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "results_df = spark.read.table(f\"{bronze_catalog}.{bronze_schema}.{bronze_table_dlt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba95b6-a829-40a2-8dc4-be90d406af97",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, FloatType\n",
    "\n",
    "# Define mapping: original column -> (new name, new data type or None if unchanged)\n",
    "column_transformations = {\n",
    "    \"RESULTID\": (\"result_id\", IntegerType()),\n",
    "    \"RACEID\": (\"race_id\", IntegerType()),\n",
    "    \"DRIVERID\": (\"driver_id\", IntegerType()),\n",
    "    \"CONSTRUCTORID\": (\"constructor_id\", IntegerType()),\n",
    "    \"F1NUM\": (\"number\", IntegerType()),  # changed to String if needed\n",
    "    \"GRID\": (\"grid\", IntegerType()),\n",
    "    \"POSITION\": (\"position\", IntegerType()),\n",
    "    \"POSITIONTEXT\": (\"position_text\", StringType()),\n",
    "    \"POSITIONORDER\": (\"position_order\", IntegerType()),\n",
    "    \"POINTS\": (\"points\", FloatType()),\n",
    "    \"LAPS\": (\"laps\", IntegerType()),\n",
    "    \"TIME\": (\"time\", StringType()),\n",
    "    \"MILLISECONDS\": (\"milliseconds\", IntegerType()),\n",
    "    \"FASTESTLAP\": (\"fastest_lap\", IntegerType()),\n",
    "    \"RANK\": (\"rank\", IntegerType()),\n",
    "    \"FASTESTLAPTIME\": (\"fastest_lap_time\", StringType()),\n",
    "    \"FASTESTLAPSPEED\": (\"fastest_lap_speed\", StringType()),\n",
    "     \"STATUSID\": (\"status_id\", IntegerType())\n",
    "}\n",
    "\n",
    "# Apply renaming and casting\n",
    "transformed_columns = [\n",
    "    col(old).cast(dtype).alias(new) if dtype else col(old).alias(new)\n",
    "    for old, (new, dtype) in column_transformations.items()\n",
    "]\n",
    "\n",
    "# Create the transformed DataFrame\n",
    "results_selected_df = results_df.select(*transformed_columns) \\\n",
    "                                    .withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24db39-fca2-4ee5-b9b3-71d44e4a223b",
   "metadata": {
    "type": "markdown"
   },
   "source": [
    "# ----------\n",
    "##### Step 2 - Write the output to processed container in delta/parquet table or parquet file format\n",
    "# ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8cd16-f147-4ddd-bee6-3b091abeb427",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "if target_type == 'file':\n",
    "    if target_format == 'parquet':\n",
    "        results_selected_df.write.mode(\"overwrite\").parquet(f\"{silver_folder_path}/results\")\n",
    "elif target_type == 'table':\n",
    "    if target_format == 'parquet':\n",
    "        results_selected_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(f\"{silver_catalog}.{silver_schema}.{silver_table_par}\")\n",
    "    elif  target_format == 'delta':\n",
    "        results_selected_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{silver_catalog}.{silver_schema}.{silver_table_dlt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20ddc9-f77a-4dba-b51c-bd83098652bd",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "if target_type == 'file':\n",
    "    if target_format == 'parquet':\n",
    "        results_read_df = spark.read.parquet(f\"{silver_folder_path}/results\")\n",
    "elif target_type == 'table':\n",
    "    if target_format == 'parquet':\n",
    "        results_read_df = spark.read.table(f\"{silver_catalog}.{silver_schema}.{silver_table_par}\")\n",
    "    elif  target_format == 'delta':\n",
    "        results_read_df = spark.read.table(f\"{silver_catalog}.{silver_schema}.{silver_table_dlt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b0eab-9743-4a63-8f85-2e054f7e6bbf",
   "metadata": {
    "type": "python"
   },
   "outputs": [],
   "source": [
    "results_read_df.show()"
   ]
  }
 ],
 "metadata": {
  "Last_Active_Cell_Index": 8,
  "kernelspec": {
   "name": "notebook"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  },
  "title": "New Untitled_1744724325561_ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
